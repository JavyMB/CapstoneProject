{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EzMRmn3qGUp"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Instalación e Importación de Librerías Necesarias\n",
        "# =====================================================\n",
        "!pip install --quiet google-cloud-bigquery pandas matplotlib seaborn scikit-learn\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from google.cloud import bigquery\n",
        "    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, PowerTransformer\n",
        "    from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
        "    from sklearn.decomposition import PCA\n",
        "    print(\"✅ Todas las librerías necesarias están instaladas.\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ Falta instalar alguna librería: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 2. Cargar Variables Secretas en Google Colab\n",
        "# =====================================================\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Obtener variables secretas (si las configuraste en Google Colab)\n",
        "PROJECT_ID = userdata.get('PROJECT_ID')  # ID de tu Proyecto de Google Cloud\n",
        "DATASET_ID = \"tracking_dataset\"  # ⚠️ Cambia esto con el nombre de tu dataset en BigQuery\n",
        "TABLE_ID = \"tracking_data\"  # ⚠️ Tabla que contiene los datos\n",
        "\n",
        "# Verificar si las variables se cargaron correctamente\n",
        "if not PROJECT_ID:\n",
        "    raise ValueError(\"❌ No se encontró PROJECT_ID en Secrets.\")\n",
        "\n",
        "print(\"✅ Variables secretas cargadas correctamente.\")"
      ],
      "metadata": {
        "id": "HiaoZCHaqtLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 3. Autenticación en Google Cloud\n",
        "# ================================\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Conectar con BigQuery\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "print(f\"✅ Conexión establecida con BigQuery en el proyecto: {PROJECT_ID}\")"
      ],
      "metadata": {
        "id": "wMx5Ql-uq3Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 4. Obtener Datos desde BigQuery\n",
        "# =====================================================\n",
        "query = f\"SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\"\n",
        "df = client.query(query).to_dataframe()\n",
        "\n",
        "# Mostrar los primeros registros\n",
        "display(df.head())\n"
      ],
      "metadata": {
        "id": "1PIpQPp2q87n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 5. Ingeniería de Características\n",
        "# =====================================================\n",
        "# Copia del DataFrame original para la transformación\n",
        "df_transformed = df.copy()\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 5.1 Generación de Nuevas Características\n",
        "# -----------------------------------------------------\n",
        "if 'battery_level' in df_transformed.columns:\n",
        "    df_transformed['battery_low'] = (df_transformed['battery_level'] < 20).astype(int)\n",
        "\n",
        "if 'signal_strength' in df_transformed.columns:\n",
        "    df_transformed['signal_quality'] = pd.cut(df_transformed['signal_strength'],\n",
        "                                              bins=[-120, -90, -70, 0],\n",
        "                                              labels=[\"Baja\", \"Media\", \"Alta\"])\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 5.2 Discretización (Binning)\n",
        "# -----------------------------------------------------\n",
        "if 'latitude' in df_transformed.columns and 'longitude' in df_transformed.columns:\n",
        "    df_transformed['location_bin'] = (df_transformed['latitude'].astype(str) + \"_\" +\n",
        "                                      df_transformed['longitude'].astype(str))\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 5.3 Codificación de Variables Categóricas\n",
        "# -----------------------------------------------------\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "categorical_cols = ['signal_quality', 'location_bin']\n",
        "\n",
        "# Verificar que las columnas existen en el DataFrame antes de aplicar la codificación\n",
        "categorical_cols = [col for col in categorical_cols if col in df_transformed.columns]\n",
        "\n",
        "if categorical_cols:\n",
        "    encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "\n",
        "    encoded_features = encoder.fit_transform(df_transformed[categorical_cols])\n",
        "    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "    df_transformed = df_transformed.drop(columns=categorical_cols)\n",
        "    df_transformed = pd.concat([df_transformed, encoded_df], axis=1)\n",
        "\n",
        "    print(f\"✅ Se han codificado las siguientes columnas: {categorical_cols}\")\n",
        "else:\n",
        "    print(\"⚠️ No se encontraron columnas categóricas para codificar.\")\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 5.4 Escalamiento de Variables Numéricas\n",
        "# -----------------------------------------------------\n",
        "scaler = StandardScaler()\n",
        "numeric_cols = df_transformed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "df_transformed[numeric_cols] = scaler.fit_transform(df_transformed[numeric_cols])\n",
        "\n",
        "# Mostrar las primeras filas después de la transformación\n",
        "display(df_transformed.head())"
      ],
      "metadata": {
        "id": "FJtuQ09Rq_fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 6. Selección y Extracción de Características\n",
        "# =====================================================\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 6.1 Eliminación de Características con Baja Varianza\n",
        "# -----------------------------------------------------\n",
        "\n",
        "# Filtrar solo columnas numéricas antes de aplicar VarianceThreshold\n",
        "numeric_cols = df_transformed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "if numeric_cols:\n",
        "    var_thresh = VarianceThreshold(threshold=0.01)\n",
        "\n",
        "    # Aplicar VarianceThreshold solo a las columnas numéricas\n",
        "    df_numeric_filtered = df_transformed[numeric_cols]\n",
        "    df_high_variance = pd.DataFrame(var_thresh.fit_transform(df_numeric_filtered),\n",
        "                                    columns=[col for i, col in enumerate(numeric_cols) if var_thresh.variances_[i] > 0.01])\n",
        "\n",
        "    print(f\"✅ Se han conservado {df_high_variance.shape[1]} características con suficiente varianza.\")\n",
        "    display(df_high_variance.head())\n",
        "else:\n",
        "    print(\"⚠️ No hay suficientes columnas numéricas para aplicar VarianceThreshold.\")\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 6.2 Selección de Características por Importancia Estadística (ANOVA)\n",
        "# -----------------------------------------------------\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "if 'status' in df_transformed.columns:\n",
        "    # Filtrar solo columnas numéricas antes de aplicar SelectKBest\n",
        "    numeric_cols = df_transformed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "    # Asegurar que la variable objetivo también es numérica\n",
        "    if df_transformed['status'].dtype == 'object':\n",
        "        df_transformed['status'] = df_transformed['status'].astype('category').cat.codes\n",
        "\n",
        "    # Separar variables predictoras (X) y variable objetivo (y)\n",
        "    X = df_transformed[numeric_cols].drop(columns=['status'], errors='ignore')\n",
        "    y = df_transformed['status']\n",
        "\n",
        "    if not X.empty:\n",
        "        selector = SelectKBest(score_func=f_classif, k=min(5, X.shape[1]))  # Selecciona hasta 5 características\n",
        "        X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "        selected_features = X.columns[selector.get_support()]\n",
        "        df_selected = pd.DataFrame(X_selected, columns=selected_features)\n",
        "\n",
        "        print(f\"✅ Se han seleccionado las {df_selected.shape[1]} características más relevantes.\")\n",
        "        display(df_selected.head())\n",
        "    else:\n",
        "        print(\"⚠️ No hay suficientes características numéricas para aplicar SelectKBest.\")\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 6.3 Reducción de Dimensionalidad con PCA\n",
        "# -----------------------------------------------------\n",
        "pca = PCA(n_components=5)\n",
        "principal_components = pca.fit_transform(df_transformed[numeric_cols])\n",
        "\n",
        "df_pca = pd.DataFrame(principal_components, columns=[f'PC{i+1}' for i in range(5)])\n",
        "display(df_pca.head())\n",
        "\n",
        "# Visualizar la varianza explicada por los componentes principales\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, 6), np.cumsum(pca.explained_variance_ratio_), marker=\"o\", linestyle=\"--\")\n",
        "plt.xlabel(\"Número de Componentes\")\n",
        "plt.ylabel(\"Varianza Acumulada\")\n",
        "plt.title(\"Varianza Explicada por PCA\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n11Ojgv4rSEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 7. Subir los Datos Transformados a BigQuery\n",
        "# =====================================================\n",
        "from pandas_gbq import to_gbq\n",
        "\n",
        "# Función para limpiar nombres de columnas y hacerlos válidos en BigQuery\n",
        "def clean_column_names(df):\n",
        "    df.columns = (df.columns\n",
        "                  .str.replace(r'\\W', '_', regex=True)  # Reemplaza cualquier carácter no alfanumérico con \"_\"\n",
        "                  .str.lower()  # Convierte todo a minúsculas\n",
        "                  .str[:290]  # Asegura que los nombres de columna no excedan 300 caracteres\n",
        "                 )\n",
        "    return df\n",
        "\n",
        "# Aplicar limpieza de nombres de columnas\n",
        "df_transformed = clean_column_names(df_transformed)\n",
        "\n",
        "# Especificar la tabla de destino en BigQuery\n",
        "table_id_transformed = f\"{DATASET_ID}.tracking_data_transformed\"\n",
        "\n",
        "# Subir los datos transformados a BigQuery\n",
        "to_gbq(df_transformed, destination_table=table_id_transformed, project_id=PROJECT_ID, if_exists=\"replace\")\n",
        "\n",
        "print(f\"✅ Datos transformados cargados en BigQuery: {table_id_transformed}\")"
      ],
      "metadata": {
        "id": "AgZ2vnXAryLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusión\n",
        "\n",
        "Aquí generamos un conjunto de datos sintéticos representativo para poder monitorear dispositivos en movimiento dentro de un área geográfica específica, en este caso Houston, TX. Este dataset nos incluye atributos clave como identificadores únicos de dispositivos y usuarios, coordenadas GPS, nivel de batería, intensidad de señal, detección de manipulación, estado del dispositivo y posibles violaciones de restricciones.\n",
        "\n",
        "El enfoque de generación aleatoria con restricciones realistas nos permite simular escenarios operativos variados, como dispositivos en movimiento, en reposo o en situaciones de incumplimiento de normas. Además, la inclusión de valores de batería y señal nos permite modelar condiciones de funcionamiento en entornos reales, facilitando el análisis de posibles fallos o pérdidas de conectividad.\n",
        "\n",
        "Este dataset sintético nos es ideal para poder realizar pruebas, simulaciones y validaciones sin necesidad de recurrir a datos reales, garantizando la privacidad de los usuarios y permitiendo la experimentación con algoritmos de análisis, visualización y predicción en el contexto del monitoreo de activos y seguridad.\n"
      ],
      "metadata": {
        "id": "gSb6-y8vQ3jP"
      }
    }
  ]
}